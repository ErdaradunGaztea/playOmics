---
title: "my-vignette"
output: rmarkdown::html_vignette
date: Sys.date()
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(playOmics)
```

```{r}
# Additional libraries
library(tidyverse)
library(readxl)
```

# Package description

how does it handles categorical data?

# Data preparation

Most often each dataset is stored in the separate file (e.g. csv or xlsx) as it comes from different teams/laboratories.

We can read them separately, giving as descriptive names as possible. At this point you should also take care of unifying identifier column name, changing incorrect columns types (e.g. char to numeric), removing unwanted variables and any other required cleaning.

Each dataset should be structured in the following format: variables in the columns, observations in the rows and, obligatory, first column with observation ID (named equally for each dataset).

```{r}
clinical_data <-
  read_excel("~/code/test_data/TCGA-LAML/csv/clinical_data.xlsx", na = c("NA", "NA,NA")) %>% 
  data.table::transpose(keep.names="ID", make.names="attrib_name") %>% 
  select(-"overallsurvival") %>% 
  mutate_at(.vars = c("years_to_birth", "overall_survival"), as.numeric)

met_HM27 <-
  read_excel("~/code/test_data/TCGA-LAML/csv/Methylation (Gene level, HM27).xlsx", na = c("NA", "NA,NA")) %>% 
  data.table::transpose(keep.names="ID", make.names="attrib_name") %>% 
  mutate_at(vars(-ID), as.numeric)

met_HM450K <-
  read_excel("~/code/test_data/TCGA-LAML/csv/Methylation (Gene level, HM450K).xlsx", na = c("NA", "NA,NA")) %>% 
   data.table::transpose(keep.names="ID", make.names="attrib_name") %>% 
  mutate_at(vars(-ID), as.numeric)

miRNA <-
  read_excel("~/code/test_data/TCGA-LAML/csv/miRNA (Gene level).xlsx", na = c("NA", "NA,NA")) %>% 
   data.table::transpose(keep.names="ID", make.names="attrib_name") %>% 
  mutate_at(vars(-ID), as.numeric)

mutation <-
  read_excel("~/code/test_data/TCGA-LAML/csv/Mutation (Gene level).xlsx", na = c("NA", "NA,NA")) %>% 
   data.table::transpose(keep.names="ID", make.names="attrib_name") %>% 
  mutate_at(vars(-ID), as.numeric)

RNAseq <-
  read_excel("~/code/test_data/TCGA-LAML/csv/RNAseq (HiSeq, Gene level).xlsx", na = c("NA", "NA,NA")) %>% 
   data.table::transpose(keep.names="ID", make.names="attrib_name") %>% 
  mutate_at(vars(-ID), as.numeric)

SCNV_log_ratio <-
  read_excel("~/code/test_data/TCGA-LAML/csv/SCNV (Focal level, log-ratio).xlsx", na = c("NA", "NA,NA")) %>% 
   data.table::transpose(keep.names="ID", make.names="attrib_name") %>% 
  mutate_at(vars(-ID), as.numeric)
```

For the LAML data (available at TCGA portal), we have 7 different datasets: clinical, RNASeq, mutation, SCNV_log_ratio, miRNA, met_HM450K, met_HM27.

# Connecting dataset

In our package we used an early integration approach (concatenating all dataframe into a one structure).

However, at this point, we cate a list of dataframes - this allows us to manipulate each dataframe individually and, at the same time, to take advantage of its common structure.

The function **connect_datasets()** allows to create a list of named dataframes. Each element of a list will receive a name of a dataframe. We can call an additional parameter *remove_original_data* indicates, whether the original dataframes should be removed. This is often needed as omics data can become quite heavy due to its dimension (tens to hunderds of thousands features)

```{r}
LAML_data <- connect_datasets(clinical_data, met_HM27, met_HM450K, miRNA, mutation, RNAseq, SCNV_log_ratio)

LAML_data
```

When calling this function, we receive a list with 7 elements with common "ID" variable at the beggining at each dataset.

# Data coverage

While conducting omics experiment, different data might be available for different modalites due to various reason (e.g. detection limit, missing samples between laboratories, incorrect material for different type of analysis etc). Therefore it is a primary need to check data coverage between different sets.

This can be easily obtained with *plot_coverage()* function:

```{r}
plot_coverage(LAML_data)
```
As for the LAML data we see that for the majority of the datasets we have all data available.

# Check your data

To discover the data structure, one can use *data_summary()* function. It presents number of samples together with number of variables and describes the content (number of numeric/character/factor columns):

```{r}
data_summary(LAML_data)
```
It's helpful to discover at glance whether the data have required structure. This might be especially important when reading data from text files (e.g. for proteomics experiment).


Nextly, we can explore data two functions:

- *check_data()* will return base statistics about each numerical variable separately. It's a simple way to check for suspicious variables (e.g. low number of unique positions) 

```{r}
check_data(LAML_data$clinical_data)
```

- *plot_density_numeric()* will draw a density plot to visualize trends in the numeric data; each 

```{r}
plot_density_numeric(LAML_data$miRNA)
```

# Check for additional effects 

PCA plots

```{r}
LAML_data$RNAseq %>%
  left_join(LAML_data$clinical_data %>% select(ID, status)) %>% 
    plot_PCA(status, "Status", "RNA data (per sample)")
```

```{r}
LAML_data$RNAseq %>%
  left_join(LAML_data$clinical_data %>% select(ID, gender)) %>% 
  select(-ID) %>% 
  plot_PCA(gender, "Sex", "RNA")
```

# Quality check

This experiment should be conducted separately for each dataframe, as each omic has its own golden standards for data preprocessing.

Two functions have been implemented:

- filter_below_threshold - user can define a numeric threshold, for which data are considered as valid in defined percentage of samples (e.g. more than 3 reads in more than 50% of samples)
```{r}
LAML_data[["RNAseq"]] <- filter_below_threshold(data = LAML_data[["RNAseq"]],
                                                numeric_threshold =  3, 
                                                pcent_of_samples = 0.5)
LAML_data[["RNAseq"]]
```
After applying this filter, data reduced from 19,5k variables to 13,991 k.

- filter missing data:

```{r}
# let's replace all zeros with NA to pretend missing data
LAML_data[["miRNA"]][LAML_data[["miRNA"]] == 0] <- NA
# apply filter
LAML_data[["miRNA"]] <- filter_missing(data = LAML_data[["miRNA"]],
                                       pcent_of_samples = 0.5)
LAML_data[["miRNA"]]
```

Initially we had 504 columns. After filtering for non-missing values in at least of 50% columns we end up with 316 columns.

# Define analysis target

Right now the playOmics package allows only for supervised classification analysis. Therefore it is crucial to define analysis target, which will be propagated to classification algorithm.

If the data is structured as described in the previous sections, then we most likely will have one dataset that contains phenotype data e.g. whether patient survived or died. With *define_target()* function we are obligated to pass a name of this dataframe (e.g. "clinical data"), a name of a column, which contains desired status and an indication of "positive" class (the one we want to predict with our analysis). One additional argument, *id_variable* indicates name of column containing samples identifiers. As discussed previously, it should be common for all of the datasets to allow data merging.

```{r}
target <-
  define_target(target_variable = "status",
              positive_class_indication = "1",
              phenotype_df = "clinical_data",
              id_variable = "ID"
              )

target
```
In next step, we want to propagate the defined target column from phenotype/clinical dataset into other datasets. This can be done using *prepare_data_for_modelling()* function. It will also prepare names to match the naming convention, transform all character and factor variables into "dummy" columns (e.g. column "sex" with two values: "male", "female" will be transformed to two columns: "sex_male" and "sex_female" filled with 1/0 values) and will translate logical columns into numbers. Observations with missing values in the target column will be removed from data.

It will allow us to filter and model data, as described in next sections.

```{r}
LAML_data_prepared <-
  prepare_data_for_modelling(data = LAML_data,
             target = target)
```

# Feature selection

In the omics experiment it is often the case that number of features is many times larger than number of observations.  Data like this are prone to overfit, as described in [Vabalas et al.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0224365#pone-0224365-g008). Therefore, before we attempt to predict the target, we need to reduce data size. We choose filter method (apart of wrapper or embedded methods) for the sake of performance.

Vabalas and his team proved that the most overfit- and data leakage-resilient method for feature selection using filter method is nested cross-validation with inner and outer  loops. This however might be computationally exhaustive as we often work with tens/hundreds of thousands features in total. Therefore we propose another method: k-fold cross-validatied ranking dedicated for feature selection.

In each fold, training data (k-1 folds) are ranked to check their convergence with target in the univariate analysis fashion. This process is repeated k times and nextly, mean rank value from all runs is calculated. Different evaluation metrics are available, as stated in [https://mlr3filters.mlr-org.com/]. Variables might be selected in three ways: by selecting defined "top n" features, by selecting % of variables from each dataset or by defining a cut-off threshold for metric.

What's important, at this level we are still operating on each dataset separately, which give us opportunity to preserve equal number of slots for all sets.

> This is a proposition and needs further tests (especially in case of data leakage - thereotically, all the data are already seen [but not all at once], which might lead to information leakage)

```{r}
LAML_data_filtered <- 
  nested_filtering(
    data = LAML_data_prepared, 
    target = target, 
    filter_name = "auc", 
    cutoff_method = "top_n", 
    cutoff_treshold = 10,
    n_threads = 10)
```

# Classification

In the current working directory, a folder is created. Experiment name must be unique.

In this step, data will finally connect into one dataframe.

LR + SVM?

```{r}
my_experiment_name <- "LAML_experiment5"

small_models <-
create_multiple_models(
  data_filtered = LAML_data_filtered,
  experiment_name = my_experiment_name,
  n_cores = 2)
```

# Check results

```{r}
results <- get_metrics_for_all_data(experiment_name = my_experiment_name, n_cores = 5)
```

```{r}
# results_GUI(results)
```

